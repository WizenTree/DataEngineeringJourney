# Data Engineering Journey

This repository documents my step-by-step journey to mastering Data Engineering. It contains daily hands-on projects that incrementally build production-grade skills across data ingestion, transformation, orchestration, storage and deployment using industry-relevant tools and technologies.

## Objectives

- Build a job-ready data engineering portfolio.
- Develop modular, testable, and maintainable ETL pipelines.
- Apply best practices in automation, orchestration, logging, and error handling.
- Prepare for real-world data engineering interviews through implementation-driven learning.

---

## Tools & Technologies Used
| Category       | Tools/Technologies   |
|----------------|----------------------|
|Programming     | Python, SQL         |
|Data Processing | Pandas, NumPy       |
|Databases       | MySQL, PostgreSQL   |
|Data Warehousing| Amazon Redshift, Google BigQuery|
|Orchestration   | Apache Airflow, cron|
|Cloud           | AWS (S3, EC2), GCP  |
|Deployment      | Docker              |
|Logging & Monitoring | Logging module, Traceback manual alerts |
|Version Control | Git, GitHub|

---

## Daily Milestone

Each folder (`Day1`,....,`DayN`) includes:
- **Topic/Focus** of the day
- **Clean and well commented code**
- **ETL scripts or pipelines**
- **README.md** with key learnings and results

> This format ensures a structured, modular progression in mastering data engineering tasks.

---

## Example Day Summary

### Day7 - Modular ETL Pipeline with Logging and Error Handling
- Broke down ETL logic into modular, reusable functions.
- Added centralized logging and traceback-based error capture.
- End-To-End tested on a local dataset using MySQL as the target.
- Created README.md

---

## Usage

To clone and explore:
```bash
git clone https://github.com/WizenTree/DataEngineeringJourney.git
cd DataEngineeringJourney/DayX
```
